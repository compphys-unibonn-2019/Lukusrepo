---
title: "Tutorial 9"
author: "Laura Zywietz Rolon"
date: "12/11/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Tutorial 9:

# Simple Markov Chain 

**1:** Consider the 2-state Markov chain generated from the stochastic matrix
\[
P =
\begin{pmatrix}
    1-\alpha & \alpha \\
    \beta & 1-\beta \\
\end{pmatrix}
\]
(c.f. example 6.1.1 in the lecture script).

* For which choices of $\alpha, \beta$ is the state space reducible/ irreducible?

*Answer:* The state space is only for $\alpha, \beta \neq 0$ irreducible. For $\alpha = \beta = 0$ it is reducible.

* Calculate the expected return time $\tau = \mathbb{E}_1(T_1)$ to state 1 (starting from state 1) depending on $\alpha$ and $\beta$ analytically.

*Answer:* One can easely deduce from the stochastic matrix that $p_{1} = 1-\alpha, p_{2} = \alpha\beta, p_3 = \alpha (1-\beta) \beta$ for the probability $p_n$ to get in $n$ steps back to state 1. Then, it follows $p_n = \alpha (1-\beta)^{(n-2)}\beta$ for the $n$th step.
The expected return time is then given by:
$$
\begin{align}
 \tau &= \mathbb{E}_1(T_1) = \sum_{n=1}^{\infty} n \cdot p_n\\
      &= (1-\alpha) + \alpha\sum_{n=2}^{\infty} n \beta (1-\beta)^{(n-2)}\\
      &= (1- \alpha) + \alpha \biggl(\frac{\beta^2 - 1}{\beta(\beta-1)}\biggl)\\
      &= 1 + \frac{\alpha}{\beta}
\end{align}
$$
 
* Implement the calculation of $\tau$ using repeated application of the transition matrix and summing up to some convergence criterion. (This version is straightforwardly generalizable to more complicated processes).

*Solution:*
The expected return time $\tau$ can be also more generally calculated by starting from an initial state, i.e. state 1 $\stackrel{^}{=} (1,0)$, then making the transition to the next state $(1, 0) \cdot P = (1-\alpha, \alpha)$, where the first entry shows the return probability after one step. To get the return probabilita after $2$ steps more generally, we project $(1-\alpha, \alpha)$ to state 2 by applying the projection matrix   
\[
Proj =
\begin{pmatrix}
    0 & 0 \\
    0 & 1\\
\end{pmatrix}
\]
to get $(0, \alpha)$. Then we multiply this with $P$ again to find in the first entry of the result the return probability after 2 steps, i.e. $\alpha\beta$. Continuing this process $n$
times, we get the return probability $p_n$ after $n$ steps. Thus, we have to sum over all first entries for every step times the current step size, to get the expected return time.
```{r}
getERT <- function(alpha, beta){
  eps = 0.01
  #starting at state 1
  mu <- matrix(c(1,0), nrow=1, ncol = 2)
  # stochastic matrix
  p <- matrix(c(1-alpha, beta, alpha, 1-beta) ,nrow=2, ncol=2)
  #projector on state 2
  proj <- matrix(c(0,0,0,1), nrow = 2, ncol = 2)
  mult <- mu
  steps <- 1
  res <- 0
  resbef <- 1
  while(abs(res-resbef) > eps){
    resbef <- res
    mult <- mult %*% p
    res <- res + steps * mult[1]
    steps <- steps + 1
    mult <- mult %*% proj
  }
 return(res)
}

#result
alpha = 0.5
beta = 0.5
print("Computed expected return time for alpha = 0.5, beta = 0.5:")
print(getERT(alpha, beta))
```
* Estimate $\tau$ by simulating the Markov chain, i.e. implement the random walk on the graph.

*Solution:*
```{r}
#simulating Markov chain

rndmWMarkov <- function(alpha, beta){
  N <- 10000
  #start from state 1
  currentstate <- 1
  returntime <- 0
  steps <- 0
  res <- 0
  p <- matrix(c(1-alpha, beta, alpha, 1-beta) ,nrow=2, ncol=2)
  
  for (i in c(1:N)) {
    prob <- runif(n=1)
    if(prob <= p[currentstate, 1]){
      currentstate = 1 
      res <- res + returntime 
      returntime <- 0
      steps <- steps + 1
    } 
    else currentstate = 2
    returntime <- returntime + 1
  }
  return(res/steps)
}

#result:
alpha = 0.5
beta = 0.5
print("Computed expected return time for alpha = 0.5, beta = 0.5:")
print(rndmWMarkov(alpha, beta))
```

* Interprete the results for the two limiting cases (a) $\alpha \to 0$ and (b) $\beta \to 0$ with $\alpha \neq 0$.

*Answer:*
We expect for $\alpha \to 1 $ and $\beta \to 0$, that the expected return time diverges, since then after arriving in state 2 we can never go back to state 1, the Markov chain becomes reducible.
```{r}
#limiting cases
alpha = 1
beta = 0
print(rndmWMarkov(alpha, beta))
```

# Ising Model

**2:** Consider the $d = 2$-dimensional Ising Model with Hamiltonian
\[ H = -\frac{1}{2}\sum_{i,j}J_{ij}s_is_j - h \sum_is_i\]
where $i,j$ run over all lattice sites and $s_i \in \lbrace -1, 1 \rbrace$. In this exercise we will only consider the case of nearest neighbour interactions on a 2-dimensional $N$ x $N$ lattice with periodic boundary conditions. The first-mentioned retriction amounts to setting $J_{ij} = J = $ const for neighbouring lattice sites and $J_{ij} = 0$ in all other cases.

* Write a function which computes $H$ for a given spin configuration $s=\lbrace s_i\rbrace$ on a 2-dimensional $N$ x $N$ lattice with periodic boundary conditions.

*Solution:*
```{r}
#spins
N <- 100
s <- rep(1, N*N)
signs <- runif(n=N*N)
s[which(signs < 0.5)] <- -1
spin <- matrix(s, nrow = N, ncol = N)

#hamiltonian
getH <- function(N, J, h, spin){
  res <- 0
  for (i in c(1:N)) {
    for (j in c(1:N)) {
      res <- sum(J*spin[i, j]*(spin[i, (j-1)%%N] + spin[i, (j+1)%%N] + spin[(i-1)%%N, j] +                   spin[(i+1)%%N, j]))
    }
  }
  H <- -0.5*res - h*sum(spin)
  return(H)
}

#result
J <- 3
h <- 2

H <- getH(N,J,h,spin)
print(H)
```

* Write a function which computes $\Delta H = H(s) - H(s')$, where $s= \lbrace s_1, ..., s_j, ... \rbrace$ and $s'= \lbrace s_1, ..., s'_j, ... \rbrace$, i.e. they differ only at a single lattice site.
```{r}
spin1 <- spin
j1 <- round(runif(n=1, min = 1, max = N))
j2 <- round(runif(n=1, min = 1, max = N))
spin1[j1, j2] <- -1* spin1[j1, j2]

H1 <- getH(N,J,h,spin1)
print("H(s') = ")
print(H1)
print("Delta H = ")
print(H - H1)
```